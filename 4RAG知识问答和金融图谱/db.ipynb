{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、chorma使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -hromadb (e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hromadb (e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'E:\\\\program\\\\anaconda\\\\envs\\\\llamaIndex\\\\Scripts\\\\chroma.exe' -> 'E:\\\\program\\\\anaconda\\\\envs\\\\llamaIndex\\\\Scripts\\\\chroma.exe.deleteme'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached chromadb-1.0.6-cp39-abi3-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: packaging>=19.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.23.1)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Using cached chromadb-1.0.6-cp39-abi3-win_amd64.whl (18.2 MB)\n",
      "Installing collected packages: chromadb\n"
     ]
    }
   ],
   "source": [
    "# ! pip install \"chromadb>=0.4.5\"\n",
    "! pip install chromadb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#! chroma run --path ../data/db --port 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='localhost',port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../model/nlp_gte_sentence. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# 指定embedding模型\n",
    "from chromadb.utils import embedding_functions\n",
    "model_path = \"../model/nlp_gte_sentence\"\n",
    "em_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name = model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建向量集合\n",
    "collection = chroma_client.create_collection(name='rag_db',\n",
    "                                             embedding_function=em_fn,\n",
    "                                             metadata={\"hnsw:space\":\"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\"在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。\",\n",
    "\"虽然单独使用特定技术如倒排文件（IVF）、乘积量化（PQ）或分层导航小世界（HNSW）通常能够带来满意的结果\",\n",
    "\"GraphRAG本质上就是RAG，只不过与一般RAG相比，其检索路径上多了一个知识图谱\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入文档\n",
    "collection.add(documents=documents,\n",
    "               ids=[\"id1\",\"id2\",\"id3\"],# 插入文档的主键，实际业务中可以使用uuid\n",
    "               metadatas=[{\"chapter\":3,\"verse\":16},\n",
    "                          {\"chapter\":4,\"verse\":5},\n",
    "                          {\"chapter\":12,\"verse\":5}]# 元数据通过metadata设定\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['id1'],\n",
       " 'embeddings': array([[ 0.3879384 , -0.14354113,  0.41287565, ..., -0.47161332,\n",
       "          0.57718426, -0.39078963]]),\n",
       " 'metadatas': [{'verse': 16, 'chapter': 3}],\n",
       " 'documents': ['在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。'],\n",
       " 'data': None,\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'embeddings']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.peek(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取某一个db\n",
    "get_collection = chroma_client.get_collection(name='rag_db',\n",
    "                                              embedding_function=em_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_result= get_collection.get(ids=['id2'],\n",
    "                                include=[\"documents\",\"embeddings\", \"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['虽然单独使用特定技术如倒排文件（IVF）、乘积量化（PQ）或分层导航小世界（HNSW）通常能够带来满意的结果']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_result['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'verse': 5, 'chapter': 4}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_result['metadatas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(id_result['embeddings']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"索引技术有哪些？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1', 'id3']],\n",
       " 'distances': None,\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'verse': 16, 'chapter': 3}, {'verse': 5, 'chapter': 12}]],\n",
       " 'documents': [['在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。',\n",
       "   'GraphRAG本质上就是RAG，只不过与一般RAG相比，其检索路径上多了一个知识图谱']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['documents', 'metadatas']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_collection.query(query_texts=query,\n",
    "                    n_results=2,\n",
    "                    include=[\"documents\",'metadatas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2不同的检索条件，过滤数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id3', 'id2']],\n",
       " 'distances': None,\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'chapter': 12, 'verse': 5}, {'chapter': 4, 'verse': 5}]],\n",
       " 'documents': [['GraphRAG本质上就是RAG，只不过与一般RAG相比，其检索路径上多了一个知识图谱',\n",
       "   '虽然单独使用特定技术如倒排文件（IVF）、乘积量化（PQ）或分层导航小世界（HNSW）通常能够带来满意的结果']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['documents', 'metadatas']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定具体的verse\n",
    "get_collection.query(query_texts=query,\n",
    "                    n_results=2,\n",
    "                    include=[\"documents\",'metadatas'],\n",
    "                    where={\"verse\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1', 'id2']],\n",
       " 'distances': [[0.45845127, 0.6852661]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'verse': 16, 'chapter': 3}, {'verse': 5, 'chapter': 4}]],\n",
       " 'documents': [['在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。',\n",
       "   '虽然单独使用特定技术如倒排文件（IVF）、乘积量化（PQ）或分层导航小世界（HNSW）通常能够带来满意的结果']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_collection.query(query_texts=[\"索引技术有哪些？\"],\n",
    "                    n_results=2,\n",
    "                    where={\"chapter\": {\"$lt\": 10}},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 文档和数据的混合检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1']],\n",
       " 'distances': [[0.45845127]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'chapter': 3, 'verse': 16}]],\n",
       " 'documents': [['在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_collection.query(query_texts=[\"索引技术有哪些？\"],\n",
    "                    n_results=2,\n",
    "                    where_document={\"$contains\":\"索引\"}) # contains关键字表明，文档中含有索引这两个字的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、milvus的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymilvus\n",
      "  Downloading pymilvus-2.5.7-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: setuptools>69 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pymilvus) (75.8.0)\n",
      "Collecting grpcio<=1.67.1,>=1.49.1 (from pymilvus)\n",
      "  Downloading grpcio-1.67.1-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pymilvus) (5.29.4)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pymilvus) (1.0.1)\n",
      "Collecting ujson>=2.0.0 (from pymilvus)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pandas>=1.2.4 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pymilvus) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
      "Downloading pymilvus-2.5.7-py3-none-any.whl (226 kB)\n",
      "Downloading grpcio-1.67.1-cp310-cp310-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 14.6 MB/s eta 0:00:00\n",
      "Downloading ujson-5.10.0-cp310-cp310-win_amd64.whl (42 kB)\n",
      "Installing collected packages: ujson, grpcio, pymilvus\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.71.0\n",
      "    Uninstalling grpcio-1.71.0:\n",
      "      Successfully uninstalled grpcio-1.71.0\n",
      "Successfully installed grpcio-1.67.1 pymilvus-2.5.7 ujson-5.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -hromadb (e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hromadb (e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'E:\\program\\anaconda\\envs\\llamaIndex\\Lib\\site-packages\\~rpc'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution -hromadb (e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "embedchain 0.1.128 requires chromadb<0.6.0,>=0.5.10, but you have chromadb 1.0.6 which is incompatible.\n",
      "grpcio-tools 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.67.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymilvus import(\n",
    "connections,\n",
    "utility,\n",
    "FieldSchema, CollectionSchema, DataType,\n",
    "Collection, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(host='127.0.0.1',port=\"19530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileds =[\n",
    "    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR,\n",
    "    is_primary=True, auto_id=False, max_length=100),\n",
    "    FieldSchema(name=\"documents\",dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"embeddings\",dtype=DataType.FLOAT_VECTOR, dim=1024),\n",
    "    FieldSchema(name=\"verse\",dtype=DataType.INT64), # 元数据信息\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_db = Collection(\"rag_db\",\n",
    "    CollectionSchema(fileds),\n",
    "    consistency_level=\"Strong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\"在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。\",\n",
    "\"虽然单独使用特定技术如倒排文件（IVF）、乘积量化（PQ）或分层导航小世界（HNSW）通常能够带来满意的结果\",\n",
    "\"GraphRAG本质上就是RAG，只不过与一般RAG相比，其检索路径上多了一个知识图谱\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yun\\AppData\\Local\\Temp\\ipykernel_60768\\2549077778.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  model = HuggingFaceEmbeddings(model_name=model_path,\n",
      "No sentence-transformers model found with name ../model/nlp_gte_sentence. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# 需要手动的将文档转换成向量，然后插入到向量集合中，需要emb模型\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "model_path = \"../model/nlp_gte_sentence\"\n",
    "model = HuggingFaceEmbeddings(model_name=model_path,\n",
    "                            model_kwargs={'device':\"cpu\"})\n",
    "embeddings= model.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [ \n",
    "[str(i) for i in range(len(documents))],  # 主键pk\n",
    "documents, # 文档\n",
    "np.array(embeddings), # embeddings是手动转化好的 \n",
    "[16,5,5], # verse元数据信息\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(insert count: 3, delete count: 0, upsert count: 0, timestamp: 457570629784698881, success count: 3, err count: 0\n"
     ]
    }
   ],
   "source": [
    "# 插入数据\n",
    "insert_result = rag_db.insert(entities)\n",
    "print(insert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Collection.flush of <Collection>:\n",
       "-------------\n",
       "<name>: rag_db\n",
       "<description>: \n",
       "<schema>: {'auto_id': False, 'description': '', 'fields': [{'name': 'pk', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}, 'is_primary': True, 'auto_id': False}, {'name': 'documents', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 512}}, {'name': 'embeddings', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}, {'name': 'verse', 'description': '', 'type': <DataType.INT64: 5>}], 'enable_dynamic_field': False}\n",
       ">"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_db.flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_db.num_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 创建索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {\n",
    "\"index_type\": \"IVF_FLAT\",# 倒排索引\n",
    "\"metric_type\":\"L2\" , #　欧式距离\n",
    "\"params\":{\"nlist\": 128}, # 聚类的数量\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_db.create_index(\"embeddings\",index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 检索\n",
    "需要确定检索的是哪个向量集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_collection = Collection(\"rag_db\")\n",
    "get_collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../model/nlp_gte_sentence. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# 指定模型路径\n",
    "model_path = \"../model/nlp_gte_sentence\"\n",
    "\n",
    "# 初始化模型\n",
    "model = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# 同步生成嵌入\n",
    "query=\"索引技术有哪些？\"\n",
    "query_emb = model.embed_documents([query])  # 输入必须是列表\n",
    "\n",
    "# 输出形状\n",
    "print(np.array(query_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_collection.search (query_emb,\n",
    "                                \"embeddings\",\n",
    "                                param={\"metric_type\": \"L2\"},\n",
    "                                limit=2,\n",
    "                                output_fields=[\"documents\",\"verse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../model/nlp_gte_sentence. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# 指定模型路径（确保路径正确）\n",
    "model_path = \"../model/nlp_gte_sentence\"\n",
    "\n",
    "# 初始化模型\n",
    "model = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# 同步生成嵌入\n",
    "query = \"你的查询文本\"\n",
    "query_emb = model.embed_documents([query])  # 输入必须是列表\n",
    "\n",
    "# 输出形状（示例：(1, 768)）\n",
    "print(np.array(query_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_collection.search(query_emb,\n",
    "                            \"embeddings\",\n",
    "                            param={\"metric_type\": \"L2\"},\n",
    "                            limit=2,\n",
    "                            output_fields=[\"documents\",\"verse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [[{'id': '0', 'distance': 421.4751281738281, 'entity': {'verse': 16, 'documents': '在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。'}}]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit: {'id': '0', 'distance': 421.4751281738281, 'entity': {'verse': 16, 'documents': '在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。'}}, documents field: 在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。\n"
     ]
    }
   ],
   "source": [
    "# 看一下检索的结果\n",
    "for hits in result:\n",
    "    for hit in hits:\n",
    "        print(f\"hit: {hit}, documents field: {hit.entity.get('documents')}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  按条件混合检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.3  按条件混合检索\n",
    "result2 = get_collection.search(query_emb,\n",
    "                                \"embeddings\",\n",
    "                                param={\"metric_type\": \"L2\"},\n",
    "                                expr=\"verse < 10\",\n",
    "                                limit=1,\n",
    "                                output_fields=[\"documents\", \"verse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit: {'id': '0', 'distance': 421.4751281738281, 'entity': {'verse': 16, 'documents': '在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。'}}, documents field: 在向量搜索领域，我们拥有多种索引方法和向量处理技术，它们使我们能够在召回率、响应时间和内存使用之间做出权衡。\n"
     ]
    }
   ],
   "source": [
    "for hits in result:\n",
    "    for hit in hits:\n",
    "        print(f\"hit: {hit}, documents field: {hit.entity.get('documents')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##　2.4 Index索引优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans2\n",
    "query = np.random.normal(size=(128,))\n",
    "dataset = np.random.normal(size=(1000, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全表扫描"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(np.linalg.norm(query - dataset, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "倒排索引IVF，kmeans分块，检索最近的块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\program\\anaconda\\envs\\llamaIndex\\lib\\site-packages\\scipy\\_lib\\_util.py:440: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "num_part = 100\n",
    "(centroids, assignments)= kmeans2(dataset, num_part, iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 128)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 61, 81, 28, 94, 98, 65,  5, 56,  5])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同的块包含不同的数据\n",
    "index = [[] for _ in range(num_part)]\n",
    "for n,k in enumerate(assignments):\n",
    "    index[k].append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 269, 386]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后在不同的块内做全表扫描"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = np.argmin(np.linalg.norm(query - centroids, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(np.linalg.norm(query - dataset[index[30]], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以取top3做全表扫描"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = np.argsort(np.linalg.norm(query - centroids, axis=1))[: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 75,  9], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_index= []\n",
    "for c in cluster_ids:\n",
    "    top3_index += index[c]\n",
    "np.argmin(np.linalg.norm(query - dataset[top3_index], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_index[66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
