{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 24 00:24:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 31%   43C    P0             44W /  170W |    2329MiB /  12288MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1396    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      3396    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A      6392    C+G   ...s\\System32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7540    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     10048    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     10184    C+G   ...ufa\\6.1.10.7\\flutter\\pc_flutter.exe      N/A      |\n",
      "|    0   N/A  N/A     10944    C+G   G:\\program\\Feishu\\app\\Feishu.exe            N/A      |\n",
      "|    0   N/A  N/A     11840    C+G   ...BrowserEngine\\BaiduNetdiskUnite.exe      N/A      |\n",
      "|    0   N/A  N/A     13972    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     18236    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     20076    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     20396    C+G   ...on\\135.0.3179.73\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     21732    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     26804    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     28488    C+G   ...ialog_cw5n1h2txyewy\\PrintDialog.exe      N/A      |\n",
      "|    0   N/A  N/A     28872    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     29676    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A     32772    C+G   F:\\program\\Snipaste\\Snipaste.exe            N/A      |\n",
      "|    0   N/A  N/A     34848    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE      N/A      |\n",
      "|    0   N/A  N/A     36300    C+G   ...\\Docker\\frontend\\Docker Desktop.exe      N/A      |\n",
      "|    0   N/A  N/A     36832    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     38476    C+G   ...1\\extracted\\runtime\\WeChatAppEx.exe      N/A      |\n",
      "|    0   N/A  N/A     38644    C+G   ...on\\135.0.3179.85\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     39024    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     41028    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE      N/A      |\n",
      "|    0   N/A  N/A     42192    C+G   ..._x64__8wekyb3d8bbwe\\WebViewHost.exe      N/A      |\n",
      "|    0   N/A  N/A     42568    C+G   ...on\\135.0.3179.73\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     43368    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     47676    C+G   F:\\program\\Typora\\Typora.exe                N/A      |\n",
      "|    0   N/A  N/A     51836    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     51948    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "|    0   N/A  N/A     53792    C+G   ...\\PyCharm 2024.1.6\\bin\\pycharm64.exe      N/A      |\n",
      "|    0   N/A  N/A     54508    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     58076    C+G   G:\\program\\Microsoft VS Code\\Code.exe       N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c56e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a23415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISABLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4522aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50bdf83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce0fc9",
   "metadata": {},
   "source": [
    "官网地址： https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731bcc39",
   "metadata": {},
   "source": [
    "# 1. modelscope调用qwen2 1.5b模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c32f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc65c256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: g:\\AI\\code\\multiRAG\\model\\Qwen-1.5-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 00:24:38,667 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('Qwen/Qwen2.5-1.5B-Instruct',local_dir='../model/Qwen-1.5-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bedb82f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer,GenerationConfig\n",
    "import torch\n",
    "model_dir =  '../model/Qwen-1.5-Instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=\"auto\",\n",
    "    device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "gen_config = GenerationConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7b24f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85421990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d745202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_k\": 20,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45453e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [108386, 104256, 6313], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"你好呀！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f34172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt,temperature=0.1,top_k=20,top_p=0.8,max_new_tokens =2048):\n",
    "    gen_config.temperature =temperature\n",
    "    gen_config.top_p=top_p\n",
    "    gen_config.top_k=top_k\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是乐于助人的助手.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    "    )\n",
    "    print(text)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    # model_inputs.inputs_ids,\n",
    "    max_new_tokens= max_new_tokens,\n",
    "    generation_config = gen_config\n",
    "    )\n",
    "    generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b8d785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是乐于助人的助手.<|im_end|>\n",
      "<|im_start|>user\n",
      "hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'hello'\n",
    "run_prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de723a11",
   "metadata": {},
   "source": [
    "# 2 huggingface transformers 调用本地开源模型chatglm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15470dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 09:47:33,586 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: g:\\AI\\code\\multiRAG\\model\\chatglm3-6b\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('ZhipuAI/chatglm3-6b',local_dir='../model/chatglm3-6b',ignore_patterns='*.bin')# ,ignore_file_pattern='*.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6276dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e475a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  '../model/chatglm3-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17843a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\program\\anaconda\\envs\\llamaIndex\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "e:\\program\\anaconda\\envs\\llamaIndex\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922e2fdba7c0493d81d98a62194b83d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,trust_remote_code = True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=torch.half, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0f3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理函数\n",
    "def chatglm(prompt,history=[],model = model,tokenizer=tokenizer):\n",
    "    response,history = model.chat(tokenizer,\n",
    "                                  prompt,\n",
    "                                  history=[],\n",
    "                                  temperature=0.1,top_p=0.8,top_k=20,\n",
    "                                  max_length = 8192)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52ee8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yun/.cache\\huggingface\\modules\\transformers_modules\\chatglm3-6b\\modeling_chatglm.py:226: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好，我是 ChatGLM3-6B，是清华大学KEG实验室和智谱AI公司共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm(\"介绍你自己\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "954ea4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 流式输出\n",
    "def chatglm_stream(prompt,history=[],model = model,tokenizer=tokenizer):\n",
    "    for data in model.stream_chat(tokenizer,\n",
    "                                  prompt,\n",
    "                                  history=[],\n",
    "                                  temperature=0.1,top_p=0.8,top_k=20,\n",
    "                                  max_length = 8192):\n",
    "        yield data[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12231e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "user_prompt = \"你好请介绍一下你自己\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae0c2c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，我是 ChatGLM3-6B，是清华大学KEG实验室和智谱AI公司共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。\n"
     ]
    }
   ],
   "source": [
    "for res in chatglm_stream(user_prompt):\n",
    "    clear_output(wait=True)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282622a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef26f221",
   "metadata": {},
   "source": [
    "# 3 langchain封装llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d863fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any,List,Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer,GenerationConfig\n",
    "\n",
    "# 继承LLM类，重写call函数\n",
    "class ChatGLM(LLM):\n",
    "    tokenizer:AutoTokenizer = None\n",
    "    model:AutoModelForCausalLM = None\n",
    "    def __init__(self,model,tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "    def _call(self, prompt:str, stop:Optional[List[str]]=None, run_manager:Optional[CallbackManagerForLLMRun]=None, **kwargs:Any):\n",
    "        response,history = self.model.chat(self.tokenizer,prompt,history=[],temperature = 0.1,top_p=0.8)\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self)->str:\n",
    "        return \"chatglm\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b61260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yun\\AppData\\Local\\Temp\\ipykernel_38728\\2172477371.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chatglm(\"你好呀！\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好👋！我是人工智能助手 ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm = ChatGLM(model,tokenizer)\n",
    "chatglm(\"你好呀！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cc345",
   "metadata": {},
   "source": [
    "# 4. 在线调用讯飞星火大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304c004",
   "metadata": {},
   "source": [
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96776a",
   "metadata": {},
   "source": [
    "# 5. 问题\n",
    "numpy的兼容性问题：pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca6c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (4.50.0)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting diffusers\n",
      "  Using cached diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from peft) (2.2.2+cu121)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: Pillow in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from diffusers) (11.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: colorama in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Using cached diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface-hub, diffusers, transformers, peft\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.29.3\n",
      "    Uninstalling huggingface-hub-0.29.3:\n",
      "      Successfully uninstalled huggingface-hub-0.29.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.0\n",
      "    Uninstalling transformers-4.50.0:\n",
      "      Successfully uninstalled transformers-4.50.0\n",
      "Successfully installed diffusers-0.33.1 huggingface-hub-0.30.2 peft-0.15.2 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "# Cannot import available module of AutoModelForCausalLM in modelscope, or related packages(['transformers', 'peft', 'diffusers'])\n",
    "# ! pip install transformers peft diffusers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4311b",
   "metadata": {},
   "source": [
    "缺少pip install sentencepiece包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0f15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddd833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "# ! pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec437d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.33.3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'E:\\program\\anaconda\\envs\\llamaIndex\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cohere 5.14.0 requires tokenizers<1,>=0.15, but you have tokenizers 0.13.3 which is incompatible.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
      "Requirement already satisfied: filelock in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.3)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from transformers==4.33.3) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (4.12.2)\n",
      "Requirement already satisfied: colorama in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from tqdm>=4.27->transformers==4.33.3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers==4.33.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers==4.33.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers==4.33.3) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (from requests->transformers==4.33.3) (2025.1.31)\n",
      "Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.6 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------- 7.6/7.6 MB 23.6 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.33.3\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers==4.33.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b17bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in e:\\program\\anaconda\\envs\\llamaindex\\lib\\site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade64203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
